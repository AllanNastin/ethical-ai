{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing REBEL-large on the AI Act"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37515afa8735226"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56bc96e1c49e2a5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:25:01.607339279Z",
     "start_time": "2024-02-09T14:25:01.603230936Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from rebel_re_model import extract_triplets"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:25:09.890809094Z",
     "start_time": "2024-02-09T14:25:06.257351088Z"
    }
   },
   "id": "a494f246357bdf52",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'A variety of AI systems can generate large quantities of synthetic content that becomes increasingly hard for humans to distinguish from human-generated and authentic content. The wide availability and increasing capabilities of those systems have a significant impact on the integrity and trust in the information ecosystem, raising new risks of misinformation and manipulation at scale, fraud, impersonation and consumer deception. In the light of those impacts, the fast technological pace and the need for new methods and techniques to trace origin of information, it is appropriate to require providers of those systems to embed technical solutions that enable marking in a machine readable format and detection that the output has been generated or manipulated by an AI system and not a human. Such techniques and methods should be sufficiently reliable, interoperable, effective and robust as far as this is technically feasible, taking into account available techniques or a combination of such techniques, such as watermarks, metadata identifications, cryptographic methods for proving provenance and authenticity of content, logging methods, fingerprints or other techniques, as may be appropriate. When implementing this obligation, providers should also take into account the specificities and the limitations of the different types of content and the relevant technological and market developments in the field, as reflected in the generally acknowledged state-of-the-art. Such techniques and methods can be implemented at the level of the system or at the level of the model, including general-purpose AI models generating content, thereby facilitating fulfilment of this obligation by the downstream provider of the AI system. To remain proportionate, it is appropriate to envisage that this marking obligation should not cover AI systems performing primarily an assistive function for standard editing or AI systems not substantially altering the input data provided by the deployer or the semantics thereof.'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sample AI Act text\n",
    "with open('sample_knowledge_graph_section.txt', 'r') as file:\n",
    "    # Read the file content into a string\n",
    "    ai_act_string = file.read()\n",
    "\n",
    "ai_act_string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:25:11.462798665Z",
     "start_time": "2024-02-09T14:25:11.459410498Z"
    }
   },
   "id": "74303c016240a7ac",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing on the sample AI Act paragraph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "174b635c886abbe2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'<s><triplet> impersonation <subj> fraud <obj> subclass of</s>'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to use the tokenizr manually since we need special tokens.\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(ai_act_string, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "extracted_text[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:25:26.355808223Z",
     "start_time": "2024-02-09T14:25:24.846934188Z"
    }
   },
   "id": "fb1eddcc67947f91",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing on a single sentence from the AI Act"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32584f49be1a5618"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'<s><triplet> synthetic content <subj> authentic content <obj> opposite of <triplet> authentic content <subj> synthetic content <obj> opposite of</s>'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on a shorter string\n",
    "ai_act_one_sentence = \"A variety of AI systems can generate large quantities of synthetic content that becomes increasingly hard for humans to distinguish from human-generated and authentic content.\"\n",
    "\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(ai_act_one_sentence,  return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "extracted_text[0]"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:35:48.231927724Z",
     "start_time": "2024-02-09T14:35:46.570867737Z"
    }
   },
   "id": "initial_id",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing on three sentences from the AI Act"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d5efb19cf43970"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'<s><triplet> impersonation <subj> fraud <obj> subclass of</s>'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_act_three_sentences = \"A variety of AI systems can generate large quantities of synthetic content that becomes increasingly hard for humans to distinguish from human-generated and authentic content. The wide availability and increasing capabilities of those systems have a significant impact on the integrity and trust in the information ecosystem, raising new risks of misinformation and manipulation at scale, fraud, impersonation and consumer deception.\"\n",
    "\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(ai_act_three_sentences,  return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "extracted_text[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:38:37.643184780Z",
     "start_time": "2024-02-09T14:38:36.076360580Z"
    }
   },
   "id": "b23fb2037192cc25",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First Impresions\n",
    "\n",
    "The model seems to only output one or two relations, regardless of the input length.\n",
    "\n",
    "## Passing in the sample paragraph one sentence at a time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cf682b24e95dc0b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ai_act_string.split(\".\")\n",
    "\n",
    "# The last sentence is empty, let's test what the model outputs\n",
    "sentences[7]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:47:32.999170265Z",
     "start_time": "2024-02-09T14:47:32.952234178Z"
    }
   },
   "id": "f13549af08b5d7b1",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><triplet> synthetic content <subj> authentic content <obj> opposite of <triplet> authentic content <subj> synthetic content <obj> opposite of</s>\n",
      "<s><triplet> impersonation <subj> fraud <obj> subclass of</s>\n",
      "<s><triplet> AI <subj> AI system <obj> studies <triplet> AI system <subj> AI <obj> studied by</s>\n",
      "<s><triplet> watermark <subj> metadata <obj> subclass of</s>\n",
      "<s><triplet> state-of-the-art <subj> technological <obj> instance of</s>\n",
      "<s><triplet> AI model <subj> model <obj> subclass of</s>\n",
      "<s><triplet> assistive function <subj> AI systems <obj> subclass of</s>\n",
      "<s><triplet> World War I <subj> World War II <obj> followed by <triplet> World War II <subj> World War I <obj> follows</s>\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence,  return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "    print(extracted_text[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T14:41:13.171730204Z",
     "start_time": "2024-02-09T14:41:01.558696046Z"
    }
   },
   "id": "783657a5e48bcce5",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hallucinaitons\n",
    "The model hallucinate relations if passed in an empty string. For example, the model outputted the triplet (World War I, followed by, World War II)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e4a33d89c06bf5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Passing in longer sections (approx 10 pages) of the AI Act "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c4bea50728ef1b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'1. High-risk AI systems shall comply with the requirements established in this Chapter, taking into account its intended purpose as well as the generally acknowledged state of the art on AI and AI related technologies. The risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements. 2a. Where a product contains an artificial intelligence system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Annex II, Section A apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements required under the Union harmonisation legislation. In ensuring the compliance of high-risk AI systems referred in paragraph 1 with the requirements set out in Chapter 2 of this Title, and in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice to integrate, as appropriate, the necessary testing and reporting processes, information and documentation they provide with regard to their product into already existing documentation and procedures required under the Union harmonisation legislation listed in Annex II, Section A. Article 9 Risk management system 1. A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems. 2. The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating. It shall comprise the following steps: (a) identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to the health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose; (b) estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse; (c) evaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61; (d) adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point a of this paragraph in accordance with the provisions of the following paragraphs. 2a. The risks referred to in this paragraph shall concern only those which may be reasonably mitigated or eliminated through the development or design of the high-risk AI system, or the provision of adequate technical information. 3. The risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interaction resulting from the combined application of the requirements set out in this Chapter 2, with a view to minimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil those requirements. 4. The risk management measures referred to in paragraph 2, point (d) shall be such that relevant residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged to be acceptable. In identifying the most appropriate risk management measures, the following shall be ensured: (a) elimination or reduction of identified risks and evaluated pursuant to paragraph 2 as far as technically feasible through adequate design and development of the high-risk AI system; (b)  where appropriate, implementation of adequate mitigation and control measures addressing risks that cannot be eliminated; (c) provision of the required information pursuant to Article 13, referred to in paragraph 2, point (b) of this Article, and, where appropriate, training to deployers. With a view to eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the deployer and the presumable context in which the system is intended to be used. 5. High-risk AI systems shall be tested for the purposes of identifying the most appropriate and targeted risk management measures. Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this Chapter. 6. Testing procedures may include testing in real world conditions in accordance with Article 54a. 7. The testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the market or the putting into service. Testing shall be made against prior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system. 8. When implementing the risk management system described in paragraphs 1 to 6, providers shall give consideration to whether in view of its intended purpose the high-risk AI system is likely to adversely impact persons under the age of 18 and, as appropriate, other vulnerable groups of people. 9. For providers of high-risk AI systems that are subject to requirements regarding internal risk management processes under relevant sectorial Union law, the aspects described in paragraphs 1 to 8 may be part of or combined with the risk management procedures established pursuant to that law. Article 10 Data and data governance 1. High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such datasets are used. 2. Training, validation and testing data sets shall be subject to appropriate data governance and management practices appropriate for the intended purpose of the AI system. Those practices shall concern in particular: (a) the relevant design choices; (aa)  data collection processes and origin of data, and in the case of personal data, the original purpose of data collection; (c) relevant data preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation; (d) the formulation of assumptions, notably with respect to the information that the data are supposed to measure and represent; (e) an assessment of the availability, quantity and suitability of the data sets that are needed; (f) examination in view of possible biases that are likely to affect the health and safety of persons, negatively impact fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations; (fa)  appropriate measures to detect, prevent and mitigate possible biases identified according to point (f); (g) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed. 3. Training, validation and testing datasets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used.  These characteristics of the data sets may be met at the level of individual data sets or a combination thereof. 4. Datasets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the high-risk AI system is intended to be used. 5. To the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with the second paragraph, point f and fa, the providers of such systems may exceptionally process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. In addition to provisions set out in the Regulation (EU) 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following conditions shall apply in order for such processing to occur: (a) the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data; (b) the special categories of personal data processed for the purpose of this paragraph are subject to technical limitations on the re-use of the personal data and state of the art security and privacy-preserving measures, including pseudonymisation; (c) the special categories of personal data processed for the purpose of this paragraph are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure only authorised persons have access to those personal data with appropriate confidentiality obligations; (d) the special categories of personal data processed for the purpose of this paragraph are not to be transmitted, transferred or otherwise accessed by other parties; (e) the special categories of personal data processed for the purpose of this paragraph are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whatever comes first; (f) the records of processing activities pursuant to Regulation (EU) 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725 includes justification why the processing of special categories of personal data was strictly necessary to detect and correct biases and this objective could not be achieved by processing other data. 6. For the development of high-risk AI systems not using techniques involving the training of models, paragraphs 2 to 5 shall apply only to the testing data sets. Article 11 Technical documentation 1. The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date. The technical documentation shall be drawn up in such a way to demonstrate that the high- risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner. For this purpose, the Commission shall establish a simplified technical documentation form targeted at the needs of small and micro enterprises. Where an SME, including start-ups, opts to provide the information required in Annex IV in a simplified manner, it shall use the form referred to in this paragraph. Notified bodies shall accept the form for the purpose of conformity assessment. 2. Where a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in paragraph 1 as well as the information required under those legal acts. 3. The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend Annex IV where necessary to ensure that, in the light of technical progress, the technical documentation provides all the necessary information to assess the compliance of the system with the requirements set out in this Chapter. Article 12 Record-keeping 1. High-risk AI systems shall technically allow for the automatic recording of events (‘logs’) over the duration of the lifetime of the system. 2. In order to ensure a level of traceability of the AI system’s functioning that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for: (i) identification of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or in a substantial modification; (ii) facilitation of the post-market monitoring referred to in Article 61; and (iii) monitoring of the operation of high-risk AI systems referred to in Article 29(4). 4. For high-risk AI systems referred to in paragraph 1, point (a) of Annex III, the logging capabilities shall provide, at a minimum: (a) recording of the period of each use of the system (start date and time and end date and time of each use); (b) the reference database against which input data has been checked by the system; (c) the input data for which the search has led to a match; (d) the identification of the natural persons involved in the verification of the results, as referred to in Article 14 (5). Article 13 Transparency and provision of information to deployers 1. High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable deployers to interpret the system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider and deployer set out in Chapter 3 of this Title. 2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users. 3. The instructions for use shall contain at least the following information: (a) the identity and the contact details of the provider and, where applicable, of its authorised representative; (b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including: (i) its intended purpose; (ii) the level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity; (iii)  any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights referred to in Article 9(2); (iiia)  where applicable, the technical capabilities and characteristics of the AI system to provide information that is relevant to explain its output; (iv)  when appropriate, its performance regarding specific persons or groups of persons on which the system is intended to be used; (v)  when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system; (va)  where applicable, information to enable deployers to interpret the system’s output and use it appropriately. (c) the changes to the high-risk AI system and its performance which have been pre- determined by the provider at the moment of the initial conformity assessment, if any; (d) the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the deployers; (e) the computational and hardware resources needed, the expected lifetime of the high- risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as regards software updates; (ea)  where relevant, a description of the mechanisms included within the AI system that allows users to properly collect, store and interpret the logs in accordance with Article 12. Article 14 Human oversight 1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use. 2. Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter. 3. The oversight measures shall be commensurate to the risks, level of autonomy and context of use of the AI system and shall be ensured through either one or all of the following types of measures: (a)  measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service; (b)  measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user. 4. For the purpose of implementing paragraphs 1 to 3, the high-risk AI system shall be provided to the user in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate to the circumstances: (a) to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, also in view of detecting and addressing anomalies, dysfunctions and unexpected performance; (b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (‘automation bias’), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons; (c) to correctly interpret the high-risk AI system’s output, taking into account for example the interpretation tools and methods available; (d) to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; (e) to intervene on the operation of the high-risk AI system or interrupt, the system through a \"stop\" button or a similar procedure that allows the system to come to a halt in a safe state. 5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting from the system unless this has been separately verified and confirmed by at least two natural persons with the necessary competence, training and authority. The requirement for a separate verification by at least two natural persons shall not apply to high risk AI systems used for the purpose of law enforcement, migration, border control or asylum, in cases where Union or national law considers the application of this requirement to be disproportionate. Article 15 Accuracy, robustness and cybersecurity 1. High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and perform consistently in those respects throughout their lifecycle. 1a. To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 of this Article and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholder and organisations such as metrology and benchmarking authorities, encourage as appropriate, the development of benchmarks and measurement methodologies. 2. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use. 3. High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems. Technical and organisational measures shall be taken towards this regard. The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans. High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (‘feedback loops’) are duly addressed with appropriate mitigation measures. 4. High-risk AI systems shall be resilient as regards to attempts by unauthorised third parties to alter their use, outputs or performance by exploiting the system vulnerabilities. The technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks. The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training dataset (‘data poisoning’), or pre-trained components used in training (‘model poisoning’), inputs designed to cause the model to make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality attacks or model flaws. '"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"ai-act-10-pages.txt\", \"r\") as file:\n",
    "    long_ai_act_string = file.read()\n",
    "\n",
    "long_ai_act_string = long_ai_act_string.replace(\"\\n\", \" \")\n",
    "long_ai_act_string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:03:17.501829126Z",
     "start_time": "2024-02-09T18:03:17.496348381Z"
    }
   },
   "id": "8320d047246db4bf",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['1',\n 'High-risk AI systems shall comply with the requirements established in this Chapter, taking into account its intended purpose as well as the generally acknowledged state of the art on AI and AI related technologies',\n 'The risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements',\n '2a',\n 'Where a product contains an artificial intelligence system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Annex II, Section A apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements required under the Union harmonisation legislation',\n 'In ensuring the compliance of high-risk AI systems referred in paragraph 1 with the requirements set out in Chapter 2 of this Title, and in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice to integrate, as appropriate, the necessary testing and reporting processes, information and documentation they provide with regard to their product into already existing documentation and procedures required under the Union harmonisation legislation listed in Annex II, Section A',\n 'Article 9 Risk management system 1',\n 'A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems',\n '2',\n 'The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating',\n 'It shall comprise the following steps: (a) identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to the health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose; (b) estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse; (c) evaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61; (d) adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point a of this paragraph in accordance with the provisions of the following paragraphs',\n '2a',\n 'The risks referred to in this paragraph shall concern only those which may be reasonably mitigated or eliminated through the development or design of the high-risk AI system, or the provision of adequate technical information',\n '3',\n 'The risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interaction resulting from the combined application of the requirements set out in this Chapter 2, with a view to minimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil those requirements',\n '4',\n 'The risk management measures referred to in paragraph 2, point (d) shall be such that relevant residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged to be acceptable',\n 'In identifying the most appropriate risk management measures, the following shall be ensured: (a) elimination or reduction of identified risks and evaluated pursuant to paragraph 2 as far as technically feasible through adequate design and development of the high-risk AI system; (b)  where appropriate, implementation of adequate mitigation and control measures addressing risks that cannot be eliminated; (c) provision of the required information pursuant to Article 13, referred to in paragraph 2, point (b) of this Article, and, where appropriate, training to deployers',\n 'With a view to eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the deployer and the presumable context in which the system is intended to be used',\n '5',\n 'High-risk AI systems shall be tested for the purposes of identifying the most appropriate and targeted risk management measures',\n 'Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this Chapter',\n '6',\n 'Testing procedures may include testing in real world conditions in accordance with Article 54a',\n '7',\n 'The testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the market or the putting into service',\n 'Testing shall be made against prior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system',\n '8',\n 'When implementing the risk management system described in paragraphs 1 to 6, providers shall give consideration to whether in view of its intended purpose the high-risk AI system is likely to adversely impact persons under the age of 18 and, as appropriate, other vulnerable groups of people',\n '9',\n 'For providers of high-risk AI systems that are subject to requirements regarding internal risk management processes under relevant sectorial Union law, the aspects described in paragraphs 1 to 8 may be part of or combined with the risk management procedures established pursuant to that law',\n 'Article 10 Data and data governance 1',\n 'High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such datasets are used',\n '2',\n 'Training, validation and testing data sets shall be subject to appropriate data governance and management practices appropriate for the intended purpose of the AI system',\n 'Those practices shall concern in particular: (a) the relevant design choices; (aa)  data collection processes and origin of data, and in the case of personal data, the original purpose of data collection; (c) relevant data preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation; (d) the formulation of assumptions, notably with respect to the information that the data are supposed to measure and represent; (e) an assessment of the availability, quantity and suitability of the data sets that are needed; (f) examination in view of possible biases that are likely to affect the health and safety of persons, negatively impact fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations; (fa)  appropriate measures to detect, prevent and mitigate possible biases identified according to point (f); (g) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed',\n '3',\n 'Training, validation and testing datasets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose',\n 'They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used',\n ' These characteristics of the data sets may be met at the level of individual data sets or a combination thereof',\n '4',\n 'Datasets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the high-risk AI system is intended to be used',\n '5',\n 'To the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with the second paragraph, point f and fa, the providers of such systems may exceptionally process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons',\n 'In addition to provisions set out in the Regulation (EU) 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following conditions shall apply in order for such processing to occur: (a) the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data; (b) the special categories of personal data processed for the purpose of this paragraph are subject to technical limitations on the re-use of the personal data and state of the art security and privacy-preserving measures, including pseudonymisation; (c) the special categories of personal data processed for the purpose of this paragraph are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure only authorised persons have access to those personal data with appropriate confidentiality obligations; (d) the special categories of personal data processed for the purpose of this paragraph are not to be transmitted, transferred or otherwise accessed by other parties; (e) the special categories of personal data processed for the purpose of this paragraph are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whatever comes first; (f) the records of processing activities pursuant to Regulation (EU) 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725 includes justification why the processing of special categories of personal data was strictly necessary to detect and correct biases and this objective could not be achieved by processing other data',\n '6',\n 'For the development of high-risk AI systems not using techniques involving the training of models, paragraphs 2 to 5 shall apply only to the testing data sets',\n 'Article 11 Technical documentation 1',\n 'The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date',\n 'The technical documentation shall be drawn up in such a way to demonstrate that the high- risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements',\n 'It shall contain, at a minimum, the elements set out in Annex IV',\n 'SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner',\n 'For this purpose, the Commission shall establish a simplified technical documentation form targeted at the needs of small and micro enterprises',\n 'Where an SME, including start-ups, opts to provide the information required in Annex IV in a simplified manner, it shall use the form referred to in this paragraph',\n 'Notified bodies shall accept the form for the purpose of conformity assessment',\n '2',\n 'Where a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in paragraph 1 as well as the information required under those legal acts',\n '3',\n 'The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend Annex IV where necessary to ensure that, in the light of technical progress, the technical documentation provides all the necessary information to assess the compliance of the system with the requirements set out in this Chapter',\n 'Article 12 Record-keeping 1',\n 'High-risk AI systems shall technically allow for the automatic recording of events (‘logs’) over the duration of the lifetime of the system',\n '2',\n 'In order to ensure a level of traceability of the AI system’s functioning that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for: (i) identification of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or in a substantial modification; (ii) facilitation of the post-market monitoring referred to in Article 61; and (iii) monitoring of the operation of high-risk AI systems referred to in Article 29(4)',\n '4',\n 'For high-risk AI systems referred to in paragraph 1, point (a) of Annex III, the logging capabilities shall provide, at a minimum: (a) recording of the period of each use of the system (start date and time and end date and time of each use); (b) the reference database against which input data has been checked by the system; (c) the input data for which the search has led to a match; (d) the identification of the natural persons involved in the verification of the results, as referred to in Article 14 (5)',\n 'Article 13 Transparency and provision of information to deployers 1',\n 'High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable deployers to interpret the system’s output and use it appropriately',\n 'An appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider and deployer set out in Chapter 3 of this Title',\n '2',\n 'High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users',\n '3',\n 'The instructions for use shall contain at least the following information: (a) the identity and the contact details of the provider and, where applicable, of its authorised representative; (b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including: (i) its intended purpose; (ii) the level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity; (iii)  any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights referred to in Article 9(2); (iiia)  where applicable, the technical capabilities and characteristics of the AI system to provide information that is relevant to explain its output; (iv)  when appropriate, its performance regarding specific persons or groups of persons on which the system is intended to be used; (v)  when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system; (va)  where applicable, information to enable deployers to interpret the system’s output and use it appropriately',\n '(c) the changes to the high-risk AI system and its performance which have been pre- determined by the provider at the moment of the initial conformity assessment, if any; (d) the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the deployers; (e) the computational and hardware resources needed, the expected lifetime of the high- risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as regards software updates; (ea)  where relevant, a description of the mechanisms included within the AI system that allows users to properly collect, store and interpret the logs in accordance with Article 12',\n 'Article 14 Human oversight 1',\n 'High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use',\n '2',\n 'Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter',\n '3',\n 'The oversight measures shall be commensurate to the risks, level of autonomy and context of use of the AI system and shall be ensured through either one or all of the following types of measures: (a)  measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service; (b)  measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user',\n '4',\n 'For the purpose of implementing paragraphs 1 to 3, the high-risk AI system shall be provided to the user in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate to the circumstances: (a) to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, also in view of detecting and addressing anomalies, dysfunctions and unexpected performance; (b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (‘automation bias’), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons; (c) to correctly interpret the high-risk AI system’s output, taking into account for example the interpretation tools and methods available; (d) to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; (e) to intervene on the operation of the high-risk AI system or interrupt, the system through a \"stop\" button or a similar procedure that allows the system to come to a halt in a safe state',\n '5',\n 'For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting from the system unless this has been separately verified and confirmed by at least two natural persons with the necessary competence, training and authority',\n 'The requirement for a separate verification by at least two natural persons shall not apply to high risk AI systems used for the purpose of law enforcement, migration, border control or asylum, in cases where Union or national law considers the application of this requirement to be disproportionate',\n 'Article 15 Accuracy, robustness and cybersecurity 1',\n 'High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and perform consistently in those respects throughout their lifecycle',\n '1a',\n 'To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 of this Article and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholder and organisations such as metrology and benchmarking authorities, encourage as appropriate, the development of benchmarks and measurement methodologies',\n '2',\n 'The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use',\n '3',\n 'High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems',\n 'Technical and organisational measures shall be taken towards this regard',\n 'The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans',\n 'High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (‘feedback loops’) are duly addressed with appropriate mitigation measures',\n '4',\n 'High-risk AI systems shall be resilient as regards to attempts by unauthorised third parties to alter their use, outputs or performance by exploiting the system vulnerabilities',\n 'The technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks',\n 'The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training dataset (‘data poisoning’), or pre-trained components used in training (‘model poisoning’), inputs designed to cause the model to make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality attacks or model flaws',\n '']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences = long_ai_act_string.split(\". \")\n",
    "preprocessed_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:08:04.567369114Z",
     "start_time": "2024-02-09T18:08:04.561335560Z"
    }
   },
   "id": "69180808bb1f9ed9",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['High-risk AI systems shall comply with the requirements established in this Chapter, taking into account its intended purpose as well as the generally acknowledged state of the art on AI and AI related technologies',\n 'The risk management system referred to in Article 9 shall be taken into account when ensuring compliance with those requirements',\n 'Where a product contains an artificial intelligence system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Annex II, Section A apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements required under the Union harmonisation legislation',\n 'In ensuring the compliance of high-risk AI systems referred in paragraph 1 with the requirements set out in Chapter 2 of this Title, and in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice to integrate, as appropriate, the necessary testing and reporting processes, information and documentation they provide with regard to their product into already existing documentation and procedures required under the Union harmonisation legislation listed in Annex II, Section A',\n 'Article 9 Risk management system 1',\n 'A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems',\n 'The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating',\n 'It shall comprise the following steps: (a) identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to the health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose; (b) estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse; (c) evaluation of other possibly arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61; (d) adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point a of this paragraph in accordance with the provisions of the following paragraphs',\n 'The risks referred to in this paragraph shall concern only those which may be reasonably mitigated or eliminated through the development or design of the high-risk AI system, or the provision of adequate technical information',\n 'The risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interaction resulting from the combined application of the requirements set out in this Chapter 2, with a view to minimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil those requirements',\n 'The risk management measures referred to in paragraph 2, point (d) shall be such that relevant residual risk associated with each hazard as well as the overall residual risk of the high-risk AI systems is judged to be acceptable',\n 'In identifying the most appropriate risk management measures, the following shall be ensured: (a) elimination or reduction of identified risks and evaluated pursuant to paragraph 2 as far as technically feasible through adequate design and development of the high-risk AI system; (b)  where appropriate, implementation of adequate mitigation and control measures addressing risks that cannot be eliminated; (c) provision of the required information pursuant to Article 13, referred to in paragraph 2, point (b) of this Article, and, where appropriate, training to deployers',\n 'With a view to eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, training to be expected by the deployer and the presumable context in which the system is intended to be used',\n 'High-risk AI systems shall be tested for the purposes of identifying the most appropriate and targeted risk management measures',\n 'Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this Chapter',\n 'Testing procedures may include testing in real world conditions in accordance with Article 54a',\n 'The testing of the high-risk AI systems shall be performed, as appropriate, at any point in time throughout the development process, and, in any event, prior to the placing on the market or the putting into service',\n 'Testing shall be made against prior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system',\n 'When implementing the risk management system described in paragraphs 1 to 6, providers shall give consideration to whether in view of its intended purpose the high-risk AI system is likely to adversely impact persons under the age of 18 and, as appropriate, other vulnerable groups of people',\n 'For providers of high-risk AI systems that are subject to requirements regarding internal risk management processes under relevant sectorial Union law, the aspects described in paragraphs 1 to 8 may be part of or combined with the risk management procedures established pursuant to that law',\n 'Article 10 Data and data governance 1',\n 'High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such datasets are used',\n 'Training, validation and testing data sets shall be subject to appropriate data governance and management practices appropriate for the intended purpose of the AI system',\n 'Those practices shall concern in particular: (a) the relevant design choices; (aa)  data collection processes and origin of data, and in the case of personal data, the original purpose of data collection; (c) relevant data preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation; (d) the formulation of assumptions, notably with respect to the information that the data are supposed to measure and represent; (e) an assessment of the availability, quantity and suitability of the data sets that are needed; (f) examination in view of possible biases that are likely to affect the health and safety of persons, negatively impact fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations; (fa)  appropriate measures to detect, prevent and mitigate possible biases identified according to point (f); (g) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed',\n 'Training, validation and testing datasets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose',\n 'They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used',\n ' These characteristics of the data sets may be met at the level of individual data sets or a combination thereof',\n 'Datasets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the high-risk AI system is intended to be used',\n 'To the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with the second paragraph, point f and fa, the providers of such systems may exceptionally process special categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons',\n 'In addition to provisions set out in the Regulation (EU) 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following conditions shall apply in order for such processing to occur: (a) the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data; (b) the special categories of personal data processed for the purpose of this paragraph are subject to technical limitations on the re-use of the personal data and state of the art security and privacy-preserving measures, including pseudonymisation; (c) the special categories of personal data processed for the purpose of this paragraph are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure only authorised persons have access to those personal data with appropriate confidentiality obligations; (d) the special categories of personal data processed for the purpose of this paragraph are not to be transmitted, transferred or otherwise accessed by other parties; (e) the special categories of personal data processed for the purpose of this paragraph are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whatever comes first; (f) the records of processing activities pursuant to Regulation (EU) 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725 includes justification why the processing of special categories of personal data was strictly necessary to detect and correct biases and this objective could not be achieved by processing other data',\n 'For the development of high-risk AI systems not using techniques involving the training of models, paragraphs 2 to 5 shall apply only to the testing data sets',\n 'Article 11 Technical documentation 1',\n 'The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date',\n 'The technical documentation shall be drawn up in such a way to demonstrate that the high- risk AI system complies with the requirements set out in this Chapter and provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements',\n 'It shall contain, at a minimum, the elements set out in Annex IV',\n 'SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner',\n 'For this purpose, the Commission shall establish a simplified technical documentation form targeted at the needs of small and micro enterprises',\n 'Where an SME, including start-ups, opts to provide the information required in Annex IV in a simplified manner, it shall use the form referred to in this paragraph',\n 'Notified bodies shall accept the form for the purpose of conformity assessment',\n 'Where a high-risk AI system related to a product, to which the legal acts listed in Annex II, section A apply, is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in paragraph 1 as well as the information required under those legal acts',\n 'The Commission is empowered to adopt delegated acts in accordance with Article 73 to amend Annex IV where necessary to ensure that, in the light of technical progress, the technical documentation provides all the necessary information to assess the compliance of the system with the requirements set out in this Chapter',\n 'Article 12 Record-keeping 1',\n 'High-risk AI systems shall technically allow for the automatic recording of events (‘logs’) over the duration of the lifetime of the system',\n 'In order to ensure a level of traceability of the AI system’s functioning that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for: (i) identification of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or in a substantial modification; (ii) facilitation of the post-market monitoring referred to in Article 61; and (iii) monitoring of the operation of high-risk AI systems referred to in Article 29(4)',\n 'For high-risk AI systems referred to in paragraph 1, point (a) of Annex III, the logging capabilities shall provide, at a minimum: (a) recording of the period of each use of the system (start date and time and end date and time of each use); (b) the reference database against which input data has been checked by the system; (c) the input data for which the search has led to a match; (d) the identification of the natural persons involved in the verification of the results, as referred to in Article 14 (5)',\n 'Article 13 Transparency and provision of information to deployers 1',\n 'High-risk AI systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable deployers to interpret the system’s output and use it appropriately',\n 'An appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider and deployer set out in Chapter 3 of this Title',\n 'High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users',\n 'The instructions for use shall contain at least the following information: (a) the identity and the contact details of the provider and, where applicable, of its authorised representative; (b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including: (i) its intended purpose; (ii) the level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity; (iii)  any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights referred to in Article 9(2); (iiia)  where applicable, the technical capabilities and characteristics of the AI system to provide information that is relevant to explain its output; (iv)  when appropriate, its performance regarding specific persons or groups of persons on which the system is intended to be used; (v)  when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the AI system; (va)  where applicable, information to enable deployers to interpret the system’s output and use it appropriately',\n '(c) the changes to the high-risk AI system and its performance which have been pre- determined by the provider at the moment of the initial conformity assessment, if any; (d) the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the deployers; (e) the computational and hardware resources needed, the expected lifetime of the high- risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as regards software updates; (ea)  where relevant, a description of the mechanisms included within the AI system that allows users to properly collect, store and interpret the logs in accordance with Article 12',\n 'Article 14 Human oversight 1',\n 'High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use',\n 'Human oversight shall aim at preventing or minimising the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular when such risks persist notwithstanding the application of other requirements set out in this Chapter',\n 'The oversight measures shall be commensurate to the risks, level of autonomy and context of use of the AI system and shall be ensured through either one or all of the following types of measures: (a)  measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service; (b)  measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user',\n 'For the purpose of implementing paragraphs 1 to 3, the high-risk AI system shall be provided to the user in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate to the circumstances: (a) to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, also in view of detecting and addressing anomalies, dysfunctions and unexpected performance; (b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (‘automation bias’), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons; (c) to correctly interpret the high-risk AI system’s output, taking into account for example the interpretation tools and methods available; (d) to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system; (e) to intervene on the operation of the high-risk AI system or interrupt, the system through a \"stop\" button or a similar procedure that allows the system to come to a halt in a safe state',\n 'For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting from the system unless this has been separately verified and confirmed by at least two natural persons with the necessary competence, training and authority',\n 'The requirement for a separate verification by at least two natural persons shall not apply to high risk AI systems used for the purpose of law enforcement, migration, border control or asylum, in cases where Union or national law considers the application of this requirement to be disproportionate',\n 'Article 15 Accuracy, robustness and cybersecurity 1',\n 'High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and perform consistently in those respects throughout their lifecycle',\n 'To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 of this Article and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholder and organisations such as metrology and benchmarking authorities, encourage as appropriate, the development of benchmarks and measurement methodologies',\n 'The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use',\n 'High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems',\n 'Technical and organisational measures shall be taken towards this regard',\n 'The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans',\n 'High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (‘feedback loops’) are duly addressed with appropriate mitigation measures',\n 'High-risk AI systems shall be resilient as regards to attempts by unauthorised third parties to alter their use, outputs or performance by exploiting the system vulnerabilities',\n 'The technical solutions aimed at ensuring the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks',\n 'The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training dataset (‘data poisoning’), or pre-trained components used in training (‘model poisoning’), inputs designed to cause the model to make a mistake (‘adversarial examples’ or ‘model evasion’), confidentiality attacks or model flaws']"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out short lines\n",
    "sentences = [string for string in preprocessed_sentences if len(string) > 7]\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:08:06.319843624Z",
     "start_time": "2024-02-09T18:08:06.315572965Z"
    }
   },
   "id": "abee85373f8645ac",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[[{'head': 'AI', 'type': 'studied by', 'tail': 'AI related technologies'},\n  {'head': 'AI related technologies', 'type': 'studies', 'tail': 'AI'}],\n [{'head': 'risk management', 'type': 'subclass of', 'tail': 'system'}],\n [{'head': 'Union harmonisation legislation',\n   'type': 'has part',\n   'tail': 'Annex II, Section A'},\n  {'head': 'Annex II, Section A',\n   'type': 'part of',\n   'tail': 'Union harmonisation legislation'},\n  {'head': 'Annex II, Section A',\n   'type': 'part of',\n   'tail': 'Union harmonisation legislation'},\n  {'head': 'Union harmonisation legislation',\n   'type': 'has part',\n   'tail': 'Annex II, Section A'}],\n [{'head': 'Union harmonisation legislation',\n   'type': 'has part',\n   'tail': 'Annex II, Section A'},\n  {'head': 'Annex II, Section A',\n   'type': 'part of',\n   'tail': 'Union harmonisation legislation'}],\n [{'head': 'Risk management system',\n   'type': 'use',\n   'tail': 'Risk management'}],\n [{'head': 'AI systems', 'type': 'use', 'tail': 'risk management'}],\n [{'head': 'continuous iterative process',\n   'type': 'facet of',\n   'tail': 'risk management system'}],\n [{'head': 'health, safety or fundamental rights',\n   'type': 'has part',\n   'tail': 'Article 61'},\n  {'head': 'Article 61',\n   'type': 'part of',\n   'tail': 'health, safety or fundamental rights'}],\n [{'head': 'design', 'type': 'part of', 'tail': 'development'}],\n [{'head': 'minimising risks',\n   'type': 'subclass of',\n   'tail': 'risk management'}],\n [{'head': 'hazard', 'type': 'studied by', 'tail': 'risk management'}],\n [{'head': 'risk management', 'type': 'studies', 'tail': 'risk'},\n  {'head': 'risk', 'type': 'studied by', 'tail': 'risk management'}],\n [{'head': 'training', 'type': 'subclass of', 'tail': 'technical knowledge'}],\n [{'head': 'risk management', 'type': 'studies', 'tail': 'risk'},\n  {'head': 'risk', 'type': 'studied by', 'tail': 'risk management'}],\n [{'head': 'high-risk', 'type': 'subclass of', 'tail': 'AI systems'}],\n [{'head': 'testing in real world conditions',\n   'type': 'subclass of',\n   'tail': 'Testing procedures'}],\n [{'head': 'testing', 'type': 'part of', 'tail': 'development process'}],\n [{'head': 'thresholds', 'type': 'facet of', 'tail': 'probabilistic'}],\n [{'head': 'risk management system', 'type': 'has part', 'tail': 'AI system'},\n  {'head': 'AI system', 'type': 'part of', 'tail': 'risk management system'}],\n [{'head': 'sectorial Union law',\n   'type': 'main subject',\n   'tail': 'sectorial'}],\n [{'head': 'Data and data governance',\n   'type': 'has part',\n   'tail': 'Article 10'}],\n [{'head': 'training of models with data',\n   'type': 'subclass of',\n   'tail': 'training'}],\n [{'head': 'data governance', 'type': 'subclass of', 'tail': 'management'}],\n [{'head': 'origin of data',\n   'type': 'subclass of',\n   'tail': 'data collection'}],\n [{'head': 'Training', 'type': 'has part', 'tail': 'validation'},\n  {'head': 'Training', 'type': 'has part', 'tail': 'testing'},\n  {'head': 'validation', 'type': 'part of', 'tail': 'Training'},\n  {'head': 'testing', 'type': 'part of', 'tail': 'Training'}],\n [{'head': 'statistical properties',\n   'type': 'has part',\n   'tail': 'groups of persons'},\n  {'head': 'groups of persons',\n   'type': 'subclass of',\n   'tail': 'statistical properties'}],\n [{'head': 'individual data set', 'type': 'subclass of', 'tail': 'data set'}],\n [{'head': 'Dataset', 'type': 'part of', 'tail': 'AI system'}],\n [{'head': 'Article 9(1) of Regulation (EU) 2016/679',\n   'type': 'part of',\n   'tail': 'Directive (EU) 2016/680'},\n  {'head': 'Directive (EU) 2016/680',\n   'type': 'has part',\n   'tail': 'Article 9(1) of Regulation (EU) 2016/679'},\n  {'head': 'Directive (EU) 2016/680',\n   'type': 'has part',\n   'tail': 'Article 10(1) of Regulation (EU) 2018/1725'},\n  {'head': 'Article 10(1) of Regulation (EU) 2018/1725',\n   'type': 'part of',\n   'tail': 'Directive (EU) 2016/680'}],\n [{'head': 'Regulation (EU) 2016/679',\n   'type': 'followed by',\n   'tail': 'Directive (EU) 2016/680'},\n  {'head': 'Directive (EU) 2016/680',\n   'type': 'follows',\n   'tail': 'Regulation (EU) 2016/679'},\n  {'head': 'Directive (EU) 2016/680',\n   'type': 'followed by',\n   'tail': 'Regulation (EU) 2018/1725'},\n  {'head': 'Regulation (EU) 2018/1725',\n   'type': 'follows',\n   'tail': 'Directive (EU) 2016/680'}],\n [{'head': 'training of models', 'type': 'facet of', 'tail': 'AI systems'}],\n [{'head': 'Article 11 Technical documentation 1',\n   'type': 'instance of',\n   'tail': 'Technical documentation'}],\n [{'head': 'technical documentation',\n   'type': 'facet of',\n   'tail': 'AI system'}],\n [{'head': 'notified bodies',\n   'type': 'subclass of',\n   'tail': 'competent authorities'}],\n [{'head': 'Annex IV', 'type': 'part of', 'tail': 'Annex'}],\n [{'head': 'technical documentation', 'type': 'has part', 'tail': 'Annex IV'},\n  {'head': 'Annex IV', 'type': 'part of', 'tail': 'technical documentation'}],\n [{'head': 'micro enterprises', 'type': 'subclass of', 'tail': 'small'}],\n [{'head': 'start-ups', 'type': 'subclass of', 'tail': 'SME'}],\n [{'head': 'conformity assessment',\n   'type': 'used by',\n   'tail': 'Notified bodies'}],\n [{'head': 'technical documentation',\n   'type': 'subclass of',\n   'tail': 'technical acts'}],\n [{'head': 'technical documentation',\n   'type': 'facet of',\n   'tail': 'technical progress'}],\n [{'head': 'Record-keeping', 'type': 'has part', 'tail': 'Record-keeping'},\n  {'head': 'Record-keeping', 'type': 'part of', 'tail': 'Record-keeping'}],\n [{'head': 'automatic recording of events',\n   'type': 'has part',\n   'tail': '‘logs’'},\n  {'head': '‘logs’',\n   'type': 'part of',\n   'tail': 'automatic recording of events'}],\n [{'head': 'Article 65(1)', 'type': 'part of', 'tail': 'Article 61'},\n  {'head': 'Article 61', 'type': 'has part', 'tail': 'Article 65(1)'},\n  {'head': 'Article 61', 'type': 'has part', 'tail': 'Article 29(4)'},\n  {'head': 'Article 29(4)', 'type': 'part of', 'tail': 'Article 61'}],\n [{'head': 'Article 14', 'type': 'has part', 'tail': 'Article 14(5)'},\n  {'head': 'Article 14(5)', 'type': 'part of', 'tail': 'Article 14'}],\n [{'head': 'Transparency and provision of information to deployers',\n   'type': 'has part',\n   'tail': 'Article 13'},\n  {'head': 'Transparency and provision of information to deployers',\n   'type': 'has part',\n   'tail': 'Article 13'},\n  {'head': 'Article 13',\n   'type': 'part of',\n   'tail': 'Transparency and provision of information to deployers'},\n  {'head': 'Article 13', 'type': 'part of', 'tail': 'Article 13'},\n  {'head': 'Article 13',\n   'type': 'part of',\n   'tail': 'Transparency and provision of information to deployers'},\n  {'head': 'Article 13', 'type': 'has part', 'tail': 'Article 13'}],\n [{'head': 'High-risk AI', 'type': 'subclass of', 'tail': 'AI'}],\n [{'head': 'provider', 'type': 'has part', 'tail': 'deployer'},\n  {'head': 'deployer', 'type': 'part of', 'tail': 'provider'}],\n [{'head': 'accessible', 'type': 'subclass of', 'tail': 'comprehensible'}],\n [{'head': 'Article 9(2)', 'type': 'part of', 'tail': 'fundamental rights'}],\n [{'head': 'hardware', 'type': 'used by', 'tail': 'computational'}],\n [{'head': 'Article 14 Human oversight 1',\n   'type': 'main subject',\n   'tail': 'Human oversight'}],\n [{'head': 'human-machine interface', 'type': 'facet of', 'tail': 'AI'}],\n [{'head': 'human oversight', 'type': 'facet of', 'tail': 'Human oversight'}],\n [{'head': 'AI system', 'type': 'part of', 'tail': 'AI system'},\n  {'head': 'AI system', 'type': 'part of', 'tail': 'AI system'}],\n [{'head': 'automatic relying',\n   'type': 'subclass of',\n   'tail': '‘automation bias’'}],\n [{'head': 'point 1(a)', 'type': 'part of', 'tail': 'Annex III'},\n  {'head': 'Annex III', 'type': 'has part', 'tail': 'point 1(a)'}],\n [{'head': 'asylum', 'type': 'facet of', 'tail': 'migration'}],\n [{'head': 'cybersecurity', 'type': 'facet of', 'tail': 'cybersecurity'}],\n [{'head': 'cybersecurity', 'type': 'subclass of', 'tail': 'AI'}],\n [{'head': 'metrology',\n   'type': 'subclass of',\n   'tail': 'measurement methodologies'}],\n [{'head': 'AI', 'type': 'studies', 'tail': 'AI systems'},\n  {'head': 'AI systems', 'type': 'studied by', 'tail': 'AI'}],\n [{'head': 'faults', 'type': 'subclass of', 'tail': 'errors'}],\n [{'head': 'Technical', 'type': 'part of', 'tail': 'organisational'},\n  {'head': 'organisational', 'type': 'has part', 'tail': 'Technical'}],\n [{'head': 'backup', 'type': 'subclass of', 'tail': 'technical redundancy'},\n  {'head': 'fail-safe',\n   'type': 'subclass of',\n   'tail': 'technical redundancy'}],\n [{'head': 'learn', 'type': 'part of', 'tail': 'AI'}],\n [{'head': 'exploiting', 'type': 'uses', 'tail': 'vulnerabilities'},\n  {'head': 'vulnerabilities', 'type': 'used by', 'tail': 'exploiting'}],\n [{'head': 'cybersecurity', 'type': 'has part', 'tail': 'risks'},\n  {'head': 'risks', 'type': 'part of', 'tail': 'cybersecurity'}],\n [{'head': 'data poisoning',\n   'type': 'subclass of',\n   'tail': 'AI specific vulnerabilities'}]]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes about 2 mins to run\n",
    "triplets = []\n",
    "from rebel_re_model import extract_triplets\n",
    "\n",
    "for sentence in sentences:\n",
    "    extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(sentence,  return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
    "    triplets.append(extract_triplets(extracted_text[0]))\n",
    "\n",
    "triplets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:51:35.156933443Z",
     "start_time": "2024-02-09T18:49:36.583759357Z"
    }
   },
   "id": "c46c8b7335ac982a",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'head': 'AI', 'type': 'studied by', 'tail': 'AI related technologies'},\n {'head': 'risk management', 'type': 'subclass of', 'tail': 'system'},\n {'head': 'Union harmonisation legislation',\n  'type': 'has part',\n  'tail': 'Annex II, Section A'},\n {'head': 'Union harmonisation legislation',\n  'type': 'has part',\n  'tail': 'Annex II, Section A'},\n {'head': 'Risk management system', 'type': 'use', 'tail': 'Risk management'},\n {'head': 'AI systems', 'type': 'use', 'tail': 'risk management'},\n {'head': 'continuous iterative process',\n  'type': 'facet of',\n  'tail': 'risk management system'},\n {'head': 'health, safety or fundamental rights',\n  'type': 'has part',\n  'tail': 'Article 61'},\n {'head': 'design', 'type': 'part of', 'tail': 'development'},\n {'head': 'minimising risks',\n  'type': 'subclass of',\n  'tail': 'risk management'},\n {'head': 'hazard', 'type': 'studied by', 'tail': 'risk management'},\n {'head': 'risk management', 'type': 'studies', 'tail': 'risk'},\n {'head': 'training', 'type': 'subclass of', 'tail': 'technical knowledge'},\n {'head': 'risk management', 'type': 'studies', 'tail': 'risk'},\n {'head': 'high-risk', 'type': 'subclass of', 'tail': 'AI systems'},\n {'head': 'testing in real world conditions',\n  'type': 'subclass of',\n  'tail': 'Testing procedures'},\n {'head': 'testing', 'type': 'part of', 'tail': 'development process'},\n {'head': 'thresholds', 'type': 'facet of', 'tail': 'probabilistic'},\n {'head': 'risk management system', 'type': 'has part', 'tail': 'AI system'},\n {'head': 'sectorial Union law', 'type': 'main subject', 'tail': 'sectorial'},\n {'head': 'Data and data governance',\n  'type': 'has part',\n  'tail': 'Article 10'},\n {'head': 'training of models with data',\n  'type': 'subclass of',\n  'tail': 'training'},\n {'head': 'data governance', 'type': 'subclass of', 'tail': 'management'},\n {'head': 'origin of data', 'type': 'subclass of', 'tail': 'data collection'},\n {'head': 'Training', 'type': 'has part', 'tail': 'validation'},\n {'head': 'statistical properties',\n  'type': 'has part',\n  'tail': 'groups of persons'},\n {'head': 'individual data set', 'type': 'subclass of', 'tail': 'data set'},\n {'head': 'Dataset', 'type': 'part of', 'tail': 'AI system'},\n {'head': 'Article 9(1) of Regulation (EU) 2016/679',\n  'type': 'part of',\n  'tail': 'Directive (EU) 2016/680'},\n {'head': 'Regulation (EU) 2016/679',\n  'type': 'followed by',\n  'tail': 'Directive (EU) 2016/680'},\n {'head': 'training of models', 'type': 'facet of', 'tail': 'AI systems'},\n {'head': 'Article 11 Technical documentation 1',\n  'type': 'instance of',\n  'tail': 'Technical documentation'},\n {'head': 'technical documentation', 'type': 'facet of', 'tail': 'AI system'},\n {'head': 'notified bodies',\n  'type': 'subclass of',\n  'tail': 'competent authorities'},\n {'head': 'Annex IV', 'type': 'part of', 'tail': 'Annex'},\n {'head': 'technical documentation', 'type': 'has part', 'tail': 'Annex IV'},\n {'head': 'micro enterprises', 'type': 'subclass of', 'tail': 'small'},\n {'head': 'start-ups', 'type': 'subclass of', 'tail': 'SME'},\n {'head': 'conformity assessment',\n  'type': 'used by',\n  'tail': 'Notified bodies'},\n {'head': 'technical documentation',\n  'type': 'subclass of',\n  'tail': 'technical acts'},\n {'head': 'technical documentation',\n  'type': 'facet of',\n  'tail': 'technical progress'},\n {'head': 'Record-keeping', 'type': 'has part', 'tail': 'Record-keeping'},\n {'head': 'automatic recording of events',\n  'type': 'has part',\n  'tail': '‘logs’'},\n {'head': 'Article 65(1)', 'type': 'part of', 'tail': 'Article 61'},\n {'head': 'Article 14', 'type': 'has part', 'tail': 'Article 14(5)'},\n {'head': 'Transparency and provision of information to deployers',\n  'type': 'has part',\n  'tail': 'Article 13'},\n {'head': 'High-risk AI', 'type': 'subclass of', 'tail': 'AI'},\n {'head': 'provider', 'type': 'has part', 'tail': 'deployer'},\n {'head': 'accessible', 'type': 'subclass of', 'tail': 'comprehensible'},\n {'head': 'Article 9(2)', 'type': 'part of', 'tail': 'fundamental rights'},\n {'head': 'hardware', 'type': 'used by', 'tail': 'computational'},\n {'head': 'Article 14 Human oversight 1',\n  'type': 'main subject',\n  'tail': 'Human oversight'},\n {'head': 'human-machine interface', 'type': 'facet of', 'tail': 'AI'},\n {'head': 'human oversight', 'type': 'facet of', 'tail': 'Human oversight'},\n {'head': 'AI system', 'type': 'part of', 'tail': 'AI system'},\n {'head': 'automatic relying',\n  'type': 'subclass of',\n  'tail': '‘automation bias’'},\n {'head': 'point 1(a)', 'type': 'part of', 'tail': 'Annex III'},\n {'head': 'asylum', 'type': 'facet of', 'tail': 'migration'},\n {'head': 'cybersecurity', 'type': 'facet of', 'tail': 'cybersecurity'},\n {'head': 'cybersecurity', 'type': 'subclass of', 'tail': 'AI'},\n {'head': 'metrology',\n  'type': 'subclass of',\n  'tail': 'measurement methodologies'},\n {'head': 'AI', 'type': 'studies', 'tail': 'AI systems'},\n {'head': 'faults', 'type': 'subclass of', 'tail': 'errors'},\n {'head': 'Technical', 'type': 'part of', 'tail': 'organisational'},\n {'head': 'backup', 'type': 'subclass of', 'tail': 'technical redundancy'},\n {'head': 'learn', 'type': 'part of', 'tail': 'AI'},\n {'head': 'exploiting', 'type': 'uses', 'tail': 'vulnerabilities'},\n {'head': 'cybersecurity', 'type': 'has part', 'tail': 'risks'},\n {'head': 'data poisoning',\n  'type': 'subclass of',\n  'tail': 'AI specific vulnerabilities'}]"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triplets is a list of lists of dictionaries\n",
    "# I want each list to only contain the first dictionary\n",
    "triplets2 = [item[0] for item in triplets]\n",
    "triplets2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T18:57:00.495296656Z",
     "start_time": "2024-02-09T18:57:00.459978079Z"
    }
   },
   "id": "40e1fd252d15f7fa",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualising the extracted relations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f72964d46cbd73ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Iterate through data to add edges to the graph\n",
    "for item in triplets2:\n",
    "    # G.add_edge(subject, obj, label=relation)\n",
    "    G.add_edge(item[\"head\"], item[\"tail\"], label=item[\"type\"])\n",
    "    \n",
    "# Initialize PyVis network\n",
    "net = Network(notebook=False, height=\"750px\", width=\"100%\")\n",
    "net.from_nx(G)\n",
    "\n",
    "# Customize the visualization\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.toggle_physics(True)\n",
    "\n",
    "# Generate and display the interactive graph\n",
    "net.show(\"knowledge_graph.html\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T19:02:37.121133777Z",
     "start_time": "2024-02-09T19:02:36.724514651Z"
    }
   },
   "id": "cfbf4407f8435daf",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d94e5c1959176065"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
